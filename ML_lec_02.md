# ML_lec_02: Linear Regression의 Hypothesis와 Cost 설명


## (1) linear hypothesis 

- "어떤 리니어한 모델이 우리의 데이터에 맞을거다!" 라는 가설을 세우는 것 
- 세상의 많은 것들은 리니어한 모델을 세울 수 있는 경우가 많음
  - 공부시간, 성적은 비례
- 즉 어떠한 리니어한 선이 우리의 데이터를 더 잘 설명할 수 있을까?


```
H(x) = Wx + b
```
- 선의 모양은 w와 b에 따라 달라짐
- hypothesis는 일차방정식이 될 것이다! 라는 가설 
- 그렇다면 어떠한 선이 우리가 가진 데이터에 가장 잘 맞는 선일까? 

## (2) cost function or loss function

![image](https://user-images.githubusercontent.com/28600272/43849535-a2525c72-9b70-11e8-808c-444702098e43.png)

- 실제 데이터와 가설이 나타내는 데이터 점들과의 거리를 비교
- (H(x) - y)^2
  - 차이를 계산하는 것이기 때문에 차이를 양수로 표현하고자 제곱해줌 
  - 또한 제곱을 함으로써 차이가 작을 때보다 클 때 페널티를 더 많이 부여 
    - 차이가 클때 값이 더 커짐으로써 차이를 더 작게 만들수 있는 중요한 역할 
- 가장 작은 cost 값을 가지는 w, b를 찾는 것이 문제 = minimize cost(w,b)

